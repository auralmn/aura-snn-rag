# ğŸ§  Aura HippocampalTransformer

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-Model%20Card-yellow)](MODEL_CARD.md)

A bio-inspired neuromorphic language model that integrates hippocampal memory systems with transformer architecture, enabling episodic memory formation and spatiotemporal learning.

## âœ¨ Key Features

- ğŸ§¬ **Bio-Inspired Architecture**: Integrates place cells, grid cells, and time cells from neuroscience
- ğŸ”„ **Episodic Memory Formation**: Real-time memory consolidation during inference
- ğŸŒŠ **Theta-Gamma Coupling**: Neural oscillation-based position encoding
- ğŸ­ **Prosody-Modulated Attention**: Emotional features influence attention mechanisms  
- ğŸ›¡ï¸ **Continual Learning**: EWC prevents catastrophic forgetting
- ğŸ”¬ **Neuromorphic Components**: Hybrid ANN-SNN architecture with Hebbian learning

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/auralmn/aura-hybrid-pre-model.git
cd aura-hybrid-pre-model

# Install dependencies
pip install -r requirements.txt

# Or use uv for faster installation
uv sync
```

### Basic Usage

```python
import torch
from src.core.hippocampal import HippocampalFormation
from src.core.language_zone.hippocampal_transformer import HippocampalTransformer
from src.training.train_hippocampal import Config
from transformers import T5Tokenizer

# Load tokenizer
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")

# Initialize model
config = Config(
    vocab_size=32000,
    embedding_dim=768,
    num_layers=12,
    num_heads=16,
    n_place_cells=2000
)

hippocampus = HippocampalFormation(
    embedding_dim=768,
    n_place_cells=2000,
    n_time_cells=100,
    n_grid_cells=200
)

model = HippocampalTransformer(config, hippocampus)

# Load checkpoint
checkpoint = torch.load("models/aura-hippocampal-transformer-mid-train.pt", map_location='cpu')
model.load_state_dict(checkpoint['model_state_dict'], strict=False)

# Generate text
model.eval()
prompt = "The future of artificial intelligence"
input_ids = tokenizer.encode(prompt, return_tensors='pt')
prosody = torch.zeros(1, input_ids.shape[1], 4)

with torch.no_grad():
    logits, memory_state = model(input_ids, prosody=prosody, use_memory=True)
```

### One-shot learning from episodic memory

Store a support example into hippocampal memory, then generate with retrieval enabled:

```python
from colab_l4_training import one_shot_memorize_text

support_text = "Quantum entanglement links particles across any distance."
mem_id = one_shot_memorize_text(support_text, tokenizer, model, hippocampus, device=torch.device('cpu'))

prompt = "Explain entanglement to a student"
input_ids = tokenizer.encode(prompt, return_tensors='pt')
with torch.no_grad():
    logits, _ = model(input_ids, use_memory=True)  # retrieves the stored episodic memory
```

### Inference Script

For stable text generation with repetition blocking:

```bash
python test_inference.py
```

## ğŸ—ï¸ Architecture

```
Input Tokens (32K vocab)
    â†“
PlaceCellSemanticEncoder
    â”œâ”€ Sparse activation (3% sparsity)
    â””â”€ 2000 place cells
    â†“
Theta-Gamma Position Encoding
    â”œâ”€ Î¸ rhythm: 8 Hz
    â””â”€ Î³ rhythm: 40 Hz
    â†“
12Ã— HippocampalTransformerLayer
    â”œâ”€ Multi-head Attention (16 heads)
    â”‚   â”œâ”€ Prosody modulation
    â”‚   â””â”€ Hippocampal memory gate
    â”œâ”€ Feed-forward (4096 dim)
    â””â”€ Layer normalization
    â†“
Language Model Head
    â†“
Output Logits (32K vocab)
```

### Hippocampal Formation

```
Spatial Processing          Temporal Processing
    â†“                            â†“
Place Cells (2000)          Time Cells (100)
Grid Cells (200)                 â†“
    â†“                       Event Sequences
Spatial Maps                     â†“
    â†“                            â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
         Episodic Memory
         (Cognitive Maps)
```

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Perplexity | 8-12 |
| Training Steps | 11,500 / 50,000 |
| Parameters | ~112M |
| Memory Retrieval Accuracy | 75-85% (top-5) |
| Inference Speed (CPU) | 2-5 tokens/sec |
| Inference Speed (GPU) | 15-30 tokens/sec |

## ğŸ¯ Training Details

### Dataset
- **Primary**: Nvidia Nemotron-CC-v2 (High-Quality subset)
- **Fallback**: WikiText-103
- **Tokenizer**: T5 SentencePiece (google/flan-t5-base)
- **Context Length**: 512 tokens

### Hyperparameters
- **Precision**: bfloat16 mixed precision
- **Batch Size**: 16
- **Learning Rate**: 3e-4 (cosine decay)
- **Optimizer**: AdamW (Î²â‚=0.9, Î²â‚‚=0.95)
- **Warmup**: 1,500 steps
- **Label Smoothing**: 0.2
- **EWC Lambda**: 0.4

### Consolidation
- **Sleep Interval**: Every 2,000 steps
- **Memory Creation**: Every 5 steps
- **Replay Buffer**: 1M samples
- **Memory Decay**: 0.03 per step

### Hardware
- **Training**: Nvidia L4 GPU (22.5GB VRAM)
- **Time**: ~175 hours for 11,500 steps
- **Inference**: CPU recommended (DirectML experimental)

## ğŸ“ Project Structure

```
aura_clean/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ hippocampal.py              # Hippocampal formation
â”‚   â”‚   â””â”€â”€ language_zone/
â”‚   â”‚       â”œâ”€â”€ hippocampal_transformer.py
â”‚   â”‚       â”œâ”€â”€ hippocampal_attention.py
â”‚   â”‚       â”œâ”€â”€ hippocampal_layer.py
â”‚   â”‚       â”œâ”€â”€ place_cell_encoder.py
â”‚   â”‚       â””â”€â”€ theta_gamma_encoding.py
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ train_hippocampal.py        # Training script
â”‚       â”œâ”€â”€ hippocampal_trainer.py      # Trainer class
â”‚       â””â”€â”€ train_wikitext2.py          # WikiText-2 training
â”œâ”€â”€ models/
â”‚   â””â”€â”€ aura-hippocampal-transformer-mid-train.pt  # Mid-training checkpoint
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_hippocampal_formation.py   # 17 tests
â”œâ”€â”€ test_inference.py                   # Inference script
â”œâ”€â”€ verify_hippocampal_model.py         # Model verification
â”œâ”€â”€ MODEL_CARD.md                       # HuggingFace model card
â””â”€â”€ README.md                           # This file
```

## ğŸ”¬ Research Applications

### Neuroscience
- Computational models of hippocampal function
- Episodic memory formation dynamics
- Spatial and temporal coding mechanisms

### Machine Learning
- Continual learning without catastrophic forgetting
- Memory-augmented neural networks
- Bio-inspired attention mechanisms
- One-shot learning from episodic memory

### Applications
- Long-form narrative generation
- Memory-augmented question answering
- Personalized language models
- Multi-modal learning with spatiotemporal grounding

## âš ï¸ Limitations

- **Early Checkpoint**: Model trained for 11,500/50,000 steps
- **Repetition**: May generate repetitive text (use temperature + blocking)
- **DirectML Issues**: Scatter operation incompatibility (use CPU/CUDA)
- **Inference Overhead**: Hippocampal operations add 15-20% latency
- **Experimental**: Research prototype, not production-ready

## ğŸ› ï¸ Development

### Running Tests

```bash
# Run hippocampal formation tests
pytest tests/test_hippocampal_formation.py -v

# Verify model checkpoint
python verify_hippocampal_model.py

# Test inference pipeline
python test_inference.py
```

### Training from Scratch

```bash
# Train on WikiText-2
python src/training/train_wikitext2.py

# Train with custom config
python src/training/train_hippocampal.py
```

### Visualization

```bash
# Generate hippocampal visualizations
python tests/test_hippocampal_visualization.py

# Interactive memory formation demo
python tests/demo_interactive_hippocampus.py
```

## ğŸ“š Citation

If you use this model in your research, please cite:

```bibtex
@software{aura_hippocampal_transformer_2025,
  title={Aura HippocampalTransformer: Bio-Inspired Neuromorphic Language Model},
  author={Aura Team},
  year={2025},
  url={https://github.com/auralmn/aura-hybrid-pre-model},
  note={Checkpoint step 11,500}
}
```

## ğŸ¤ Contributing

We welcome contributions! Areas of interest:

- **Model Training**: Continue training to 50,000 steps
- **Multi-modal**: Vision + language integration
- **Benchmarks**: Continual learning evaluations
- **Optimization**: DirectML compatibility, inference speed
- **Documentation**: Tutorials, examples, explanations

Please open an issue or submit a pull request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **Model Card**: [MODEL_CARD.md](MODEL_CARD.md)
- **Repository**: [github.com/auralmn/aura-hybrid-pre-model](https://github.com/auralmn/aura-hybrid-pre-model)
- **Issues**: [github.com/auralmn/aura-hybrid-pre-model/issues](https://github.com/auralmn/aura-hybrid-pre-model/issues)
- **HuggingFace**: Coming soon

## ğŸ™ Acknowledgments

Built on principles from:
- Hippocampal formation neuroscience (O'Keefe & Nadel, 1978)
- Memory-augmented neural networks (Graves et al., 2014)
- Elastic weight consolidation (Kirkpatrick et al., 2017)
- Transformer architectures (Vaswani et al., 2017)
- T5 tokenization (Raffel et al., 2020)

## ğŸŒŸ Aura Initiative

**Aura** - A leader in neuromorphic computing and hybrid AI

*Bridging neuroscience and artificial intelligence for the next generation of cognitive systems.*

---

**Status**: ğŸš§ Research Prototype | **Version**: 0.1-alpha | **Checkpoint**: 11,500/50,000 steps
