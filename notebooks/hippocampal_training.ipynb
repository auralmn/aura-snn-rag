{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hippocampal Transformer Training on Wikitext-2\n",
                "\n",
                "This notebook trains a biologically-inspired Hippocampal Transformer with:\n",
                "- **Theta-Gamma Positional Encoding** (8Hz/40Hz oscillations)\n",
                "- **Place Cell Semantic Encoder** (sparse 3% activity)\n",
                "- **Prosody-Modulated Attention** with episodic memory\n",
                "- **Wake/Sleep Phase Training** with replay consolidation\n",
                "- **EWC (Elastic Weight Consolidation)** for continual learning\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q datasets transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload Source Files\n",
                "\n",
                "Upload these files from your local `aura_clean` directory:\n",
                "1. `src/core/hippocampal.py`\n",
                "2. `src/core/language_zone/theta_gamma_encoding.py`\n",
                "3. `src/core/language_zone/place_cell_encoder.py`\n",
                "4. `src/core/language_zone/hippocampal_attention.py`\n",
                "5. `src/core/language_zone/hippocampal_layer.py`\n",
                "6. `src/core/language_zone/hippocampal_transformer.py`\n",
                "7. `src/training/hippocampal_trainer.py`\n",
                "\n",
                "Or run the cell below to use Google Drive:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Mount Google Drive (if you've uploaded files there)\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# If files are in Drive, add to path:\n",
                "# import sys\n",
                "# sys.path.insert(0, '/content/drive/MyDrive/aura_clean')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 2: Manual file upload\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Create directory structure\n",
                "!mkdir -p src/core/language_zone src/training\n",
                "\n",
                "print(\"Please upload the source files using the file browser on the left -->\")\n",
                "print(\"Or use the upload button below:\")\n",
                "# uploaded = files.upload()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all components\n",
                "import sys\n",
                "sys.path.insert(0, '/content')\n",
                "\n",
                "from src.core.hippocampal import HippocampalFormation\n",
                "from src.core.language_zone.hippocampal_transformer import HippocampalTransformer\n",
                "from src.training.hippocampal_trainer import HippocampalTransformerTrainer\n",
                "\n",
                "print(\"All components imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration & Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dataclasses import dataclass\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from datasets import load_dataset\n",
                "from transformers import GPT2Tokenizer\n",
                "import math\n",
                "\n",
                "@dataclass\n",
                "class Config:\n",
                "    # Model Config\n",
                "    vocab_size: int = 50257\n",
                "    embedding_dim: int = 384\n",
                "    num_layers: int = 4\n",
                "    num_heads: int = 6\n",
                "    dropout: float = 0.1\n",
                "    max_seq_len: int = 128\n",
                "    intermediate_size: int = 1536\n",
                "    \n",
                "    # Hippocampal\n",
                "    theta_frequency: float = 8.0\n",
                "    gamma_frequency: float = 40.0\n",
                "    n_place_cells: int = 800\n",
                "    \n",
                "    # Training\n",
                "    batch_size: int = 32\n",
                "    lr: float = 3e-4\n",
                "    max_steps: int = 2000\n",
                "    sleep_interval: int = 500\n",
                "    sleep_steps: int = 10\n",
                "    eval_interval: int = 100\n",
                "\n",
                "config = Config()\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "print(\"Loading Wikitext-2...\")\n",
                "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
                "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"Train samples: {len(dataset['train'])}\")\n",
                "print(f\"Validation samples: {len(dataset['validation'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data loading utilities\n",
                "def create_batches(dataset, tokenizer, config, split='train', max_batches=None):\n",
                "    \"\"\"Yield batches of tokenized text.\"\"\"\n",
                "    texts = [item['text'] for item in dataset[split] if len(item['text'].strip()) > 10]\n",
                "    \n",
                "    batch_count = 0\n",
                "    for i in range(0, len(texts), config.batch_size):\n",
                "        if max_batches and batch_count >= max_batches:\n",
                "            break\n",
                "            \n",
                "        batch_texts = texts[i:i+config.batch_size]\n",
                "        if len(batch_texts) < config.batch_size:\n",
                "            continue\n",
                "        \n",
                "        encoded = tokenizer(\n",
                "            batch_texts,\n",
                "            max_length=config.max_seq_len,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        input_ids = encoded['input_ids']\n",
                "        labels = input_ids.clone()\n",
                "        prosody = torch.rand(input_ids.size(0), input_ids.size(1), 4)\n",
                "        \n",
                "        batch_count += 1\n",
                "        yield input_ids, labels, prosody"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "print(\"Initializing Hippocampal Transformer...\")\n",
                "\n",
                "hippocampus = HippocampalFormation(\n",
                "    config.embedding_dim,\n",
                "    config.n_place_cells,\n",
                "    50,\n",
                "    100\n",
                ")\n",
                "\n",
                "model = HippocampalTransformer(config, hippocampus).to(device)\n",
                "trainer = HippocampalTransformerTrainer(model, config, hippocampus)\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "print(f\"Total parameters: {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Training tracking\n",
                "losses = []\n",
                "eval_losses = []\n",
                "perplexities = []\n",
                "steps = []\n",
                "\n",
                "print(\"Starting training...\")\n",
                "global_step = 0\n",
                "\n",
                "# Create training data generator\n",
                "train_gen = create_batches(dataset, tokenizer, config, 'train', max_batches=config.max_steps)\n",
                "\n",
                "# Progress bar\n",
                "pbar = tqdm(total=config.max_steps, desc=\"Training\")\n",
                "\n",
                "for input_ids, labels, prosody in train_gen:\n",
                "    global_step += 1\n",
                "    trainer.step_counter()\n",
                "    \n",
                "    input_ids = input_ids.to(device)\n",
                "    labels = labels.to(device)\n",
                "    prosody = prosody.to(device)\n",
                "    \n",
                "    if trainer.phase == \"wake\":\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        logits, place_activity = model(input_ids, prosody=prosody)\n",
                "        loss = nn.CrossEntropyLoss()(\n",
                "            logits.view(-1, config.vocab_size),\n",
                "            labels.view(-1)\n",
                "        )\n",
                "        \n",
                "        trainer.replay_buffer.add(input_ids, labels, loss.item())\n",
                "        \n",
                "        if trainer.ewc.fisher:\n",
                "            loss += trainer.ewc.penalty(model)\n",
                "        \n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        losses.append(loss.item())\n",
                "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'phase': 'Wake'})\n",
                "        \n",
                "        # Evaluation\n",
                "        if global_step % config.eval_interval == 0:\n",
                "            model.eval()\n",
                "            eval_loss = 0\n",
                "            eval_batches = 0\n",
                "            \n",
                "            with torch.no_grad():\n",
                "                for eval_input, eval_labels, eval_prosody in create_batches(dataset, tokenizer, config, 'validation', max_batches=20):\n",
                "                    eval_input = eval_input.to(device)\n",
                "                    eval_labels = eval_labels.to(device)\n",
                "                    eval_prosody = eval_prosody.to(device)\n",
                "                    \n",
                "                    eval_logits, _ = model(eval_input, prosody=eval_prosody)\n",
                "                    batch_loss = nn.CrossEntropyLoss()(\n",
                "                        eval_logits.view(-1, config.vocab_size),\n",
                "                        eval_labels.view(-1)\n",
                "                    )\n",
                "                    eval_loss += batch_loss.item()\n",
                "                    eval_batches += 1\n",
                "            \n",
                "            avg_eval_loss = eval_loss / eval_batches\n",
                "            perplexity = math.exp(avg_eval_loss)\n",
                "            \n",
                "            eval_losses.append(avg_eval_loss)\n",
                "            perplexities.append(perplexity)\n",
                "            steps.append(global_step)\n",
                "            \n",
                "            print(f\"\\nStep {global_step}: Eval Loss={avg_eval_loss:.4f}, Perplexity={perplexity:.2f}\")\n",
                "            model.train()\n",
                "    \n",
                "    elif trainer.phase == \"sleep\":\n",
                "        pbar.set_postfix({'phase': 'Sleep'})\n",
                "        print(f\"\\nSleep phase at step {global_step}\")\n",
                "        \n",
                "        if not trainer.ewc.fisher and len(trainer.replay_buffer) > 0:\n",
                "            mock_loader = []\n",
                "            samples = trainer.replay_buffer.sample(10)\n",
                "            for item in samples:\n",
                "                mock_loader.append((\n",
                "                    item[0].unsqueeze(0).to(device),\n",
                "                    item[1].unsqueeze(0).to(device)\n",
                "                ))\n",
                "            trainer.ewc.compute_fisher(mock_loader, device=device)\n",
                "        \n",
                "        replay_losses = []\n",
                "        for _ in range(config.sleep_steps):\n",
                "            optimizer.zero_grad()\n",
                "            loss = trainer.train_step_sleep()\n",
                "            if loss is not None:\n",
                "                loss.backward()\n",
                "                optimizer.step()\n",
                "                replay_losses.append(loss.item())\n",
                "        \n",
                "        avg_replay = sum(replay_losses) / len(replay_losses) if replay_losses else 0\n",
                "        print(f\"Replay Loss: {avg_replay:.4f}\")\n",
                "        \n",
                "        trainer.phase = \"wake\"\n",
                "    \n",
                "    pbar.update(1)\n",
                "    \n",
                "    if global_step >= config.max_steps:\n",
                "        break\n",
                "\n",
                "pbar.close()\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Results & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Training loss\n",
                "axes[0].plot(losses, alpha=0.3)\n",
                "# Smooth\n",
                "window = 50\n",
                "if len(losses) > window:\n",
                "    smoothed = [sum(losses[max(0,i-window):i+1])/min(i+1,window) for i in range(len(losses))]\n",
                "    axes[0].plot(smoothed, linewidth=2, label='Smoothed')\n",
                "axes[0].set_xlabel('Step')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Perplexity\n",
                "axes[1].plot(steps, perplexities, marker='o', linewidth=2)\n",
                "axes[1].set_xlabel('Step')\n",
                "axes[1].set_ylabel('Perplexity')\n",
                "axes[1].set_title('Validation Perplexity')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal Metrics:\")\n",
                "print(f\"  Best Perplexity: {min(perplexities):.2f}\")\n",
                "print(f\"  Final Perplexity: {perplexities[-1]:.2f}\")\n",
                "print(f\"  Replay Buffer Size: {len(trainer.replay_buffer)}\")\n",
                "print(f\"  Episodic Memories: {len(hippocampus.episodic_memories)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "torch.save({\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'config': config,\n",
                "    'perplexity': perplexities[-1] if perplexities else None\n",
                "}, 'hippocampal_transformer.pt')\n",
                "\n",
                "print(\"Model saved to hippocampal_transformer.pt\")\n",
                "print(\"Download it using: files.download('hippocampal_transformer.pt')\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text Generation (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate text sample\n",
                "model.eval()\n",
                "\n",
                "prompt = \"The history of artificial intelligence\"\n",
                "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
                "prosody = torch.rand(1, input_ids.size(1), 4).to(device)\n",
                "\n",
                "print(f\"Prompt: {prompt}\")\n",
                "print(\"\\nGenerated continuation:\")\n",
                "\n",
                "generated = input_ids\n",
                "for _ in range(50):  # Generate 50 tokens\n",
                "    with torch.no_grad():\n",
                "        logits, _ = model(generated, prosody=prosody)\n",
                "        next_token = logits[0, -1].argmax()\n",
                "        generated = torch.cat([generated, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
                "        prosody = torch.cat([prosody, torch.rand(1, 1, 4).to(device)], dim=1)\n",
                "\n",
                "print(tokenizer.decode(generated[0]))"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}