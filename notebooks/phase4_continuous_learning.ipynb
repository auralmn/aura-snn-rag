{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Continuous Learning\n",
    "- Enable continuous learning orchestrator (memory-only or with processor).\n",
    "- Auto-ingest `vocab_src` feeds (JSONL/CSV) into hippocampal memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch datasets transformers sentencepiece tqdm\n",
    "import torch, sys, asyncio\n",
    "from pathlib import Path\n",
    "ROOT = Path('/content/repo') if Path('/content/repo').exists() else Path.cwd().parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "from colab_l4_training import get_test_config, main, ingest_jsonl_to_memory, ingest_csv_pairs_to_memory\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with continuous learning enabled (memory-only orchestrator)\n",
    "config = get_test_config()\n",
    "config.enable_continuous_learning = True\n",
    "model, losses, cl_orch = main(config_preset='test')\n",
    "hippocampus = model.hippocampus\n",
    "print('Orchestrator available:', cl_orch is not None)\n",
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base', legacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual ingestion of vocab_src corpora (optional, besides orchestrator)\n",
    "jsonl_paths = [\n",
    "    ROOT/'vocab_src/instruct_55k_clean.jsonl',\n",
    "    ROOT/'vocab_src/OpenThoughts-114k.jsonl',\n",
    "    ROOT/'vocab_src/principles_texts.jsonl',\n",
    "]\n",
    "for p in jsonl_paths:\n",
    "    if p.exists():\n",
    "        stored = ingest_jsonl_to_memory(str(p), tokenizer, model, hippocampus, device, max_items=500)\n",
    "        print(p.name, 'stored', stored)\n",
    "\n",
    "csv_path = ROOT/'vocab_src/timeline_conversations.csv'\n",
    "if csv_path.exists():\n",
    "    stored = ingest_csv_pairs_to_memory(str(csv_path), tokenizer, model, hippocampus, device, max_items=500)\n",
    "    print(csv_path.name, 'stored', stored)\n",
    "\n",
    "print('Total memories:', hippocampus.memory_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start/stop the orchestrator (memory-only) to continuously ingest feeds\n",
    "async def run_orchestrator(duration_sec=120):\n",
    "    if cl_orch is None:\n",
    "        print('No orchestrator built (enable_continuous_learning must be True)')\n",
    "        return\n",
    "    await cl_orch.start()\n",
    "    await asyncio.sleep(duration_sec)\n",
    "    await cl_orch.stop()\n",
    "\n",
    "# To run: await run_orchestrator(120)\n",
    "# Note: In notebooks, run this in an async cell or use asyncio.run in a fresh kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
